# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for
# license information.
#
# Code generated by Microsoft (R) AutoRest Code Generator 2.3.33.0
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.serialization import Model


class RunConfiguration(Model):
    """A class to manage Run Configuration for experiment runs targeting different
    compute targets in Azure Machine Learning.

    :param script: The relative path to the python script file. The file path
     is relative to the source_directory passed to submit run.
    :type script: str
    :param command: Command to run, including arguments
    :type command: str
    :param use_absolute_path: Set true to use absolute path in docker image
     for Script.
    :type use_absolute_path: bool
    :param arguments: Command line arguments for the python script file or
     command
    :type arguments: list[str]
    :param framework: The supported frameworks are Python, PySpark, CNTK,
     TensorFlow, and PyTorch. Use Tensorflow for AmlCompute clusters, and
     Python for distributed training jobs. Possible values include: 'Python',
     'PySpark', 'Cntk', 'TensorFlow', 'PyTorch'
    :type framework: str or ~_restclient.models.Framework
    :param communicator: The supported communicators are None,
     ParameterServer, OpenMpi, and IntelMpi Keep in mind that OpenMpi requires
     a custom image with OpenMpi installed.
     Use ParameterServer or OpenMpi for AmlCompute clusters. Use IntelMpi for
     distributed training jobs. Possible values include: 'None',
     'ParameterServer', 'Gloo', 'Mpi', 'Nccl', 'ParallelTask'
    :type communicator: str or ~_restclient.models.Communicator
    :param target: Target refers to compute where the job is scheduled for
     execution. The default target is "local" referring to the local machine.
    :type target: str
    :param data_references: All the data sources are made available to the run
     during execution based on each configuration.
    :type data_references: dict[str,
     ~_restclient.models.DataReferenceConfiguration]
    :param data: All the dataset inputs that will be used in this run.
    :type data: dict[str, ~_restclient.models.Data]
    :param output_data: All the configurations that describes what to
     upload/write to the specified destinations.
    :type output_data: dict[str, ~_restclient.models.OutputData]
    :param job_name: This is primarily intended for notebooks to override the
     default job name.
     Defaults to ArgumentVector[0] if not specified.
    :type job_name: str
    :param max_run_duration_seconds: Maximum allowed time for the run. The
     system will attempt to automatically cancel the run if it took longer than
     this value.
     MaxRunDurationSeconds=null means infinite duration.
    :type max_run_duration_seconds: long
    :param node_count: Number of compute nodes to run the job on. Only applies
     to AMLCompute.
    :type node_count: int
    :param priority: Job priority for scheduling policy. Only applies to
     AMLCompute.
    :type priority: int
    :param credential_passthrough: Flag to enable remote compute to run with
     user credentials.
     If set to True, user credentials will be used to generate OBO tokens to
     access storage.
     Only applies to AMLCompute.
    :type credential_passthrough: bool
    :param identity: Identity Configuration to be used for the run.
     This field configures the type of Identity used by run when executing or
     remote compute.
    :type identity: ~_restclient.models.IdentityConfiguration
    :param environment: The environment definition, This field configures the
     python environment.
     It can be configured to use an existing Python environment or configured
     to setup a temp environment for the experiment.
     The definition is also responsible for setting the required application
     dependencies.
    :type environment: ~_restclient.models.EnvironmentDefinition
    :param history: This section is used to disable and enable experiment
     history logging features.
    :type history: ~_restclient.models.HistoryConfiguration
    :param spark: When the platform is set to Pyspark, The spark configuration
     is used to set the default sparkconf for the submitted job.
    :type spark: ~_restclient.models.SparkConfiguration
    :param parallel_task: When the framework is set to parallel task, The
     parallel configuration is used to set
     the values of parallelTasksSettings to submit job to amlcompute.
    :type parallel_task: ~_restclient.models.ParallelTaskConfiguration
    :param ai_super_computer: The attribute is used to configure details of
     the compute target to be created during experiment.
     The configuration only takes effect when the target is set to
     "aisupercomputer".
    :type ai_super_computer: ~_restclient.models.AISuperComputerConfiguration
    :param kubernetes_compute: The attribute is used to configure details of
     the compute target to be created during experiment. The configuration only
     takes effect when the target is set to "kubernetescompute".
    :type kubernetes_compute: ~_restclient.models.KubernetesCompute
    :param tensorflow: The attribute is used to configure the distributed
     tensorflow parameters.
     This attribute takes effect only when the framework is set to TensorFlow,
     and the communicator to ParameterServer.
     AmlCompute is the only supported compute for this configuration.
    :type tensorflow: ~_restclient.models.TensorflowConfiguration
    :param mpi: The attribute is used to configure the distributed MPI job
     parameters.
     This attribute takes effect only when the framework is set to Python, and
     the communicator to OpenMpi or IntelMpi.
     AmlCompute is the only supported compute type for this configuration.
    :type mpi: ~_restclient.models.MpiConfiguration
    :param py_torch: The attribute is used to configure the distributed
     PyTorch job parameters.
     This attribute takes effect when the framework is set to PyTorch and the
     communicator to Nccl or Gloo.
     AmlCompute is the only supported compute type for this configuration.
    :type py_torch: ~_restclient.models.PyTorchConfiguration
    :param hdi: This attribute takes effect only when the target is set to an
     Azure HDI compute.
     The HDI Configuration is used to set the YARN deployment mode. It is
     defaulted to cluster mode.
    :type hdi: ~_restclient.models.HdiConfiguration
    :param docker: This section is used to specify Docker Runtime properties
     for the run.
    :type docker: ~_restclient.models.DockerConfiguration
    :param command_return_code_config: Configure how command return code is
     interpreted
    :type command_return_code_config:
     ~_restclient.models.CommandReturnCodeConfig
    :param environment_variables: Environment variables to be defined in the
     compute nodes
    :type environment_variables: dict[str, str]
    :param application_endpoints: Application endpoints.
     This configuration is for AML K8S, Singularity and AML Compute to
     enable application endpoints during job submission in Dev Plat v2.
    :type application_endpoints: dict[str,
     ~_restclient.models.ApplicationEndpointConfiguration]
    """

    _attribute_map = {
        'script': {'key': 'script', 'type': 'str'},
        'command': {'key': 'command', 'type': 'str'},
        'use_absolute_path': {'key': 'useAbsolutePath', 'type': 'bool'},
        'arguments': {'key': 'arguments', 'type': '[str]'},
        'framework': {'key': 'framework', 'type': 'Framework'},
        'communicator': {'key': 'communicator', 'type': 'Communicator'},
        'target': {'key': 'target', 'type': 'str'},
        'data_references': {'key': 'dataReferences', 'type': '{DataReferenceConfiguration}'},
        'data': {'key': 'data', 'type': '{Data}'},
        'output_data': {'key': 'outputData', 'type': '{OutputData}'},
        'job_name': {'key': 'jobName', 'type': 'str'},
        'max_run_duration_seconds': {'key': 'maxRunDurationSeconds', 'type': 'long'},
        'node_count': {'key': 'nodeCount', 'type': 'int'},
        'priority': {'key': 'priority', 'type': 'int'},
        'credential_passthrough': {'key': 'credentialPassthrough', 'type': 'bool'},
        'identity': {'key': 'identity', 'type': 'IdentityConfiguration'},
        'environment': {'key': 'environment', 'type': 'EnvironmentDefinition'},
        'history': {'key': 'history', 'type': 'HistoryConfiguration'},
        'spark': {'key': 'spark', 'type': 'SparkConfiguration'},
        'parallel_task': {'key': 'parallelTask', 'type': 'ParallelTaskConfiguration'},
        'ai_super_computer': {'key': 'aiSuperComputer', 'type': 'AISuperComputerConfiguration'},
        'kubernetes_compute': {'key': 'kubernetesCompute', 'type': 'KubernetesCompute'},
        'tensorflow': {'key': 'tensorflow', 'type': 'TensorflowConfiguration'},
        'mpi': {'key': 'mpi', 'type': 'MpiConfiguration'},
        'py_torch': {'key': 'pyTorch', 'type': 'PyTorchConfiguration'},
        'hdi': {'key': 'hdi', 'type': 'HdiConfiguration'},
        'docker': {'key': 'docker', 'type': 'DockerConfiguration'},
        'command_return_code_config': {'key': 'commandReturnCodeConfig', 'type': 'CommandReturnCodeConfig'},
        'environment_variables': {'key': 'environmentVariables', 'type': '{str}'},
        'application_endpoints': {'key': 'applicationEndpoints', 'type': '{ApplicationEndpointConfiguration}'},
    }

    def __init__(self, script=None, command=None, use_absolute_path=None, arguments=None, framework=None, communicator=None, target=None, data_references=None, data=None, output_data=None, job_name=None, max_run_duration_seconds=None, node_count=None, priority=None, credential_passthrough=None, identity=None, environment=None, history=None, spark=None, parallel_task=None, ai_super_computer=None, kubernetes_compute=None, tensorflow=None, mpi=None, py_torch=None, hdi=None, docker=None, command_return_code_config=None, environment_variables=None, application_endpoints=None):
        super(RunConfiguration, self).__init__()
        self.script = script
        self.command = command
        self.use_absolute_path = use_absolute_path
        self.arguments = arguments
        self.framework = framework
        self.communicator = communicator
        self.target = target
        self.data_references = data_references
        self.data = data
        self.output_data = output_data
        self.job_name = job_name
        self.max_run_duration_seconds = max_run_duration_seconds
        self.node_count = node_count
        self.priority = priority
        self.credential_passthrough = credential_passthrough
        self.identity = identity
        self.environment = environment
        self.history = history
        self.spark = spark
        self.parallel_task = parallel_task
        self.ai_super_computer = ai_super_computer
        self.kubernetes_compute = kubernetes_compute
        self.tensorflow = tensorflow
        self.mpi = mpi
        self.py_torch = py_torch
        self.hdi = hdi
        self.docker = docker
        self.command_return_code_config = command_return_code_config
        self.environment_variables = environment_variables
        self.application_endpoints = application_endpoints
