# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for
# license information.
#
# Code generated by Microsoft (R) AutoRest Code Generator 2.3.33.0
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.serialization import Model


class OptimizationPerformanceMetrics(Model):
    """The model optimization performance metrics.

    :param original_inference_latency_ms: The original model inference time in
     milliseconds.
    :type original_inference_latency_ms: float
    :param onnx_pretuning_inference_latency_ms: The onnx model inference time
     in milliseconds before performance tuning.
    :type onnx_pretuning_inference_latency_ms: float
    :param onnx_optimal_inference_latency_ms: The onnx model inference time in
     milliseconds after performance tuning.
    :type onnx_optimal_inference_latency_ms: float
    """

    _attribute_map = {
        'original_inference_latency_ms': {'key': 'originalInferenceLatencyMs', 'type': 'float'},
        'onnx_pretuning_inference_latency_ms': {'key': 'onnxPretuningInferenceLatencyMs', 'type': 'float'},
        'onnx_optimal_inference_latency_ms': {'key': 'onnxOptimalInferenceLatencyMs', 'type': 'float'},
    }

    def __init__(self, original_inference_latency_ms=None, onnx_pretuning_inference_latency_ms=None, onnx_optimal_inference_latency_ms=None):
        super(OptimizationPerformanceMetrics, self).__init__()
        self.original_inference_latency_ms = original_inference_latency_ms
        self.onnx_pretuning_inference_latency_ms = onnx_pretuning_inference_latency_ms
        self.onnx_optimal_inference_latency_ms = onnx_optimal_inference_latency_ms
